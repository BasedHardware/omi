---
title: "Conversation Post-Processing"
description: "This document outlines the post-processing workflow for conversations in the Omi application, including high-quality transcription and speaker identification."
---

## Overview

Post-processing improves conversation quality by:
- Re-transcribing audio with a higher-quality model (WhisperX)
- Identifying speakers using speech profiles
- Optionally analyzing emotions in the recording

## Process Flow

```
POST /v1/conversations/{id}/post-processing
                    ↓
            Upload audio to GCS
                    ↓
         FAL.ai WhisperX transcription
                    ↓
         Speech profile matching
                    ↓
         Update conversation in Firestore
                    ↓
         Re-process conversation (re-extract structured data)
                    ↓
         (Optional) Emotional analysis via Hume API
```

![Post Processing](/images/docs/developer/images/postprocessing.png)

## Detailed Steps

### 1. Post-Processing Request

- App sends POST request to `/v1/conversations/{conversation_id}/post-processing`
- Request includes:
  - Audio recording for post-processing
  - Flag for emotional analysis (`emotional_feedback`)

### 2. Request Handling

- `postprocess_conversation` function in `routers/postprocessing.py` processes the request
- Retrieves existing conversation data from Firebase Firestore using `database/conversations.py`

### 3. Audio Pre-Processing and Storage

#### User Permission Check
- Checks if user allows audio storage (`database/users.py`)
- If permitted, audio uploaded to `conversations_recordings_bucket` in Google Cloud Storage

#### Audio Upload for Processing
- Audio uploaded to `postprocessing_audio_bucket` in Google Cloud Storage
- Handled by `utils/other/storage.py`

#### Cleanup
- Background thread started to delete uploaded audio after set time (e.g., 5 minutes)

### 4. FAL.ai WhisperX Transcription

- `fal_whisperx` function in `utils/stt/pre_recorded.py` sends audio to FAL.ai
- WhisperX model performs:
  - High-quality transcription
  - Speaker diarization (separating speakers)
- Returns list of transcribed words with speaker labels

### 5. Transcript Post-Processing

`fal_postprocessing` function in `utils/stt/pre_recorded.py`:
- Cleans transcript data
- Groups words into segments based on speaker and timing
- Converts segments to `TranscriptSegment` objects

### 6. Speech Profile Matching

`get_speech_profile_matching_predictions` in `utils/stt/speech_profile.py`:
- Downloads user's speech profile and known people profiles
- Uses Speechbrain model to compare speaker embeddings
- Updates segments with:
  - `is_user`: Boolean indicating if the device owner is speaking
  - `person_id`: ID of matched person from user's contacts

### 7. Conversation Update and Reprocessing

- Conversation object updated with improved transcript and speaker identification
- Updated data saved to Firebase Firestore
- If FAL.ai transcription successful:
  - `process_conversation` in `utils/conversations/process_conversation.py` re-processes
  - Re-extracts structured data (title, overview, action items, events)
  - Re-generates vector embeddings
  - Updates conversation in Pinecone

### 8. Emotional Analysis (Optional)

If `emotional_feedback=true`:
- `process_user_emotion` function called asynchronously
- Uses Hume API to analyze user's emotions in the recording
- Can trigger notifications based on detected emotions

---

## Post-Processing Status

The conversation's `postprocessing` field tracks status:

| Field | Type | Description |
|-------|------|-------------|
| `status` | enum | `not_started`, `in_progress`, `completed`, `canceled`, `failed` |
| `model` | enum | Model used (e.g., `fal_whisperx`) |
| `fail_reason` | string | Reason for failure (if applicable) |

---

## Key Code Components

```python
# In routers/postprocessing.py
@router.post("/v1/conversations/{conversation_id}/post-processing")
def postprocess_conversation(conversation_id: str, file: UploadFile, emotional_feedback: bool = False):
    # ... (request handling and pre-processing)
    words = fal_whisperx(audio_url)
    segments = fal_postprocessing(words)
    segments = get_speech_profile_matching_predictions(uid, segments)
    # ... (conversation update and reprocessing)
    if emotional_feedback:
        asyncio.create_task(process_user_emotion(uid, file_path))

# In utils/stt/pre_recorded.py
def fal_whisperx(audio_url: str):
    # ... (FAL.ai API call and processing)

def fal_postprocessing(words: List[dict]) -> List[TranscriptSegment]:
    # ... (clean and format transcript data)

# In utils/stt/speech_profile.py
def get_speech_profile_matching_predictions(uid: str, segments: List[TranscriptSegment]):
    # ... (speaker identification logic)
```

---

## Key File Locations

| Component | File Path |
|-----------|-----------|
| **Post-processing Router** | `backend/routers/postprocessing.py` |
| **FAL.ai WhisperX** | `backend/utils/stt/pre_recorded.py` |
| **Speech Profile Matching** | `backend/utils/stt/speech_profile.py` |
| **Storage Utils** | `backend/utils/other/storage.py` |
| **Process Conversation** | `backend/utils/conversations/process_conversation.py` |

---

## Related Documentation

- [Storing Conversations](/doc/developer/backend/StoringConversations) - How conversations are stored
- [Transcription](/doc/developer/backend/transcription) - Real-time transcription details
