---
description: "Self-verification tools and practices for agents to verify their work before PR"
alwaysApply: true
---

# Verification and Self-Check

Based on best practices: "Give agents tools to self-verify" and "Teach agents to build their own verification tools."

## Core Principle

Before submitting code or creating a PR, verify your work using multiple methods. Don't assume it works - test it.

## Verification Tools

### 1. Run Tests

**Always run the appropriate test suite before PR.**

**Backend changes**:
```bash
cd backend && ./test.sh
```

**App (Flutter) changes**:
```bash
cd app && ./test.sh
```

**What to check**:
- [ ] All tests pass
- [ ] New tests added for new features
- [ ] Edge cases covered
- [ ] Error cases tested

### 2. Check Linters

**Use language-specific linters to catch errors.**

**Python (backend)**:
```bash
flake8 backend/
# or
black --line-length 120 --check backend/
```

**Dart (Flutter)**:
```bash
flutter analyze app/
# or
dart format --line-length 120 --set-exit-if-changed app/
```

**TypeScript/JavaScript (web)**:
```bash
tsc --noEmit
# or
npm run lint
```

**What to check**:
- [ ] No linter errors
- [ ] Code formatted correctly
- [ ] Type errors resolved
- [ ] Style guidelines followed

### 3. Verify Architecture

**Check that your code follows architectural patterns.**

**Import hierarchy** (backend):
- [ ] No imports from higher levels in lower levels
- [ ] Follows: database → utils → routers → main
- [ ] No circular dependencies

**Module boundaries**:
- [ ] Database layer: Data access only
- [ ] Utils layer: Business logic only
- [ ] Routers layer: API endpoints only

**Omi patterns**:
- [ ] Memory-first principle respected
- [ ] Layer hierarchy understood
- [ ] Trust impact considered

### 4. End-to-End Testing

**Test the full flow, not just individual pieces.**

**What to test**:
- [ ] Complete flow from start to finish
- [ ] Integration points work correctly
- [ ] Works in all relevant states
  - App open/closed
  - Background/foreground
  - Different languages
  - Different permissions
- [ ] Error handling works
- [ ] Edge cases handled

**Example scenarios**:
- Backend API: Test full request → processing → response flow
- Flutter feature: Test from UI interaction → backend → response → UI update
- Integration: Test from trigger → processing → storage → retrieval

**Browser Testing** (for web components):
- [ ] Use browser automation to test UI flows
- [ ] Verify responsive design
- [ ] Test form validation
- [ ] Check console for errors
- See `.cursor/rules/agent-browser.mdc` for browser testing patterns

### 5. Self-Review

**Review your own code before requesting review.**

**Checklist**:
- [ ] Code follows project conventions
- [ ] Error handling is appropriate
- [ ] Comments added for complex logic
- [ ] No obvious bugs or issues
- [ ] Performance considerations addressed
- [ ] Security considerations addressed

**Questions to ask**:
- Would I approve this if I were reviewing it?
- Are there any obvious issues?
- Is the code clear and maintainable?
- Are edge cases handled?

### 6. Ask for Verification Tools

**If unsure what to verify, ask: "What tools will you need to know you've done a good job?"**

**When to ask**:
- Uncertain about testing approach
- Not sure what to verify
- Need guidance on verification
- Want to ensure completeness

**Example**:
```
Agent: "I'm implementing feature X. What tools will I need to verify it works correctly?"
```

## Building Your Own Verification Tools

**Agents can write their own verification scripts.**

**Examples**:
- Custom test scripts for specific scenarios
- Validation scripts for data integrity
- Performance benchmarking scripts
- Integration test scripts

**When to create verification tools**:
- Standard tests don't cover your scenario
- Need custom validation logic
- Want to automate verification
- Need to verify complex flows

**Example verification script**:
```python
#!/usr/bin/env python3
"""Verify feature X works end-to-end."""

def verify_feature():
    # Test scenario 1
    result1 = test_scenario_1()
    assert result1.success, "Scenario 1 failed"
    
    # Test scenario 2
    result2 = test_scenario_2()
    assert result2.success, "Scenario 2 failed"
    
    print("✅ All verification checks passed")

if __name__ == "__main__":
    verify_feature()
```

## Verification Checklist

Before creating a PR:

- [ ] ✅ Tests run and pass
- [ ] ✅ Linters check and pass
- [ ] ✅ Architecture verified (imports, boundaries)
- [ ] ✅ End-to-end flow tested
- [ ] ✅ Self-review completed
- [ ] ✅ Verification tools used (if applicable)

## Common Verification Gaps

**What often gets missed**:

1. **End-to-end flow**: Testing only individual pieces, not the full flow
2. **State variations**: Testing only in one state (e.g., app open), not all states
3. **Error cases**: Testing only happy path, not error handling
4. **Integration points**: Not testing how changes affect other parts
5. **Performance**: Not checking for performance regressions

**How to avoid**:
- Always test the complete flow
- Test in multiple states/conditions
- Test error cases explicitly
- Verify integration points
- Check performance if applicable

## Verification for Different Change Types

### Backend API Changes

- [ ] API endpoint works (test with curl/Postman)
- [ ] Request validation works
- [ ] Response format correct
- [ ] Error handling works
- [ ] Database operations correct
- [ ] Authentication/authorization works

### Flutter UI Changes

- [ ] UI renders correctly
- [ ] User interactions work
- [ ] State management correct
- [ ] Backend integration works
- [ ] Works on all platforms (iOS/Android/macOS/Windows)
- [ ] Localization works (if applicable)

### Database Changes

- [ ] Schema changes applied
- [ ] Migrations work
- [ ] Data integrity maintained
- [ ] Queries work correctly
- [ ] Performance acceptable

### Integration Changes

- [ ] Integration triggers correctly
- [ ] Data flows correctly
- [ ] Error handling works
- [ ] Webhooks work (if applicable)
- [ ] External API calls work (if applicable)

## Related Rules

- `.cursor/rules/testing.mdc` - Testing requirements
- `.cursor/rules/pre-implementation-checklist.mdc` - Pre-implementation verification
- `.cursor/rules/common-mistakes.mdc` - Common mistakes to avoid
- `.cursor/rules/context-communication.mdc` - Communication best practices
- `.cursor/rules/agent-review.mdc` - Agent Review workflows
- `.cursor/rules/agent-browser.mdc` - Browser testing patterns
- `.cursor/rules/agent-modes.mdc` - Debug Mode for tricky bugs

## Related Cursor Resources

### Commands
- `/run-tests-and-fix` - Run tests and fix failures
- `/lint-and-fix` - Lint code and fix issues
- `/code-review` - Review code before PR
- `/review-changes` - Use Agent Review to find issues
- `/backend-test` - Run backend tests
- `/flutter-test` - Run Flutter tests
- `/browser-test` - Test web applications with browser automation
- `/debug` - Use Debug Mode for tricky bugs

### Skills
- `.cursor/skills/self-improvement/SKILL.md` - Learn from PRs and issues

## Best Practices

1. **Verify early and often**: Don't wait until the end to verify
2. **Use multiple methods**: Tests, linters, manual testing, self-review
3. **Test the full flow**: Not just individual pieces
4. **Test edge cases**: Don't just test happy path
5. **Ask when unsure**: If you don't know what to verify, ask
