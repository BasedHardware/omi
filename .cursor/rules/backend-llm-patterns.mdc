---
description: "OpenAI LLM client prompt engineering LangGraph agentic system conversation processing memory extraction chat routing tool calling"
globs:
  - "backend/utils/llm/**/*.py"
  - "backend/utils/retrieval/**/*.py"
alwaysApply: false
references:
  - docs/doc/developer/backend/chat_system.mdx
  - docs/doc/developer/backend/backend_deepdive.mdx
  - backend/README.md
---

# Backend LLM Patterns

## LLM Client Configuration

### Client Setup

```python
from utils.llm.clients import get_openai_client, get_embedding_model

# Get OpenAI client
client = get_openai_client()

# Get embedding model
embedding_model = get_embedding_model()  # text-embedding-3-large
```

### Model Selection

```python
# Chat models
CHAT_MODEL = "gpt-4o"  # Primary chat model
CHAT_MODEL_MINI = "gpt-4o-mini"  # For classification

# Embedding model
EMBEDDING_MODEL = "text-embedding-3-large"
```

## Conversation Processing

### Extract Structured Data

```python
from utils.llm.conversation_processing import _get_structured

# Extract title, overview, action items, events
structured = await _get_structured(
    transcript=transcript,
    language="en",
)

# Returns:
# {
#     "title": "Meeting about project",
#     "overview": "Discussed project timeline...",
#     "action_items": [...],
#     "events": [...],
# }
```

### Extract Memories

```python
from utils.llm.conversation_processing import _extract_memories

# Extract user facts from conversation
memories = await _extract_memories(
    transcript=transcript,
    existing_memories=existing_memories,
)

# Returns list of memory objects
```

### Should Discard

```python
from utils.llm.conversation_processing import _should_discard

# Determine if conversation should be discarded
should_discard = await _should_discard(transcript)

# Returns True if conversation is noise, test, etc.
```

## Chat System

### LangGraph Router

```python
from utils.retrieval.graph import requires_context

# Classify question
needs_context = requires_context(messages)

if not needs_context:
    # Simple path - direct LLM response
    response = await simple_chat_response(messages)
elif is_persona_question(messages):
    # Persona path - use app's persona prompt
    response = await persona_chat_response(messages, app_id)
else:
    # Agentic path - full tool access
    response = await agentic_chat_response(messages, uid)
```

### Tool System

```python
from utils.retrieval.tools import get_available_tools

# Get tools for user
tools = get_available_tools(uid)

# Tools include:
# - get_conversations_tool
# - search_conversations_tool
# - get_memories_tool
# - get_calendar_events_tool
# - get_gmail_messages_tool
# - app-specific tools
```

### Agentic Response

```python
from utils.retrieval.agentic import get_agentic_response

# Get response with tool calls
response = await get_agentic_response(
    messages=messages,
    uid=uid,
    tools=available_tools,
)

# LangGraph ReAct agent:
# 1. LLM decides which tools to call
# 2. Tools execute
# 3. Results gathered
# 4. LLM generates response with citations
```

## Prompt Engineering

### System Prompts

```python
SYSTEM_PROMPT = """You are Omi, an AI assistant that helps users manage their conversations and memories.

You have access to:
- User's conversations and memories
- Calendar events
- Action items
- External integrations (Gmail, GitHub, etc.)

Always cite sources using [1][2] format when referencing conversations.
"""
```

### Conversation Extraction Prompt

```python
EXTRACTION_PROMPT = """Extract structured information from this conversation:

1. Title: Short, descriptive title
2. Overview: Concise summary
3. Action Items: Tasks mentioned
4. Events: Calendar-worthy events

Conversation:
{transcript}
"""
```

### Memory Extraction Prompt

```python
MEMORY_EXTRACTION_PROMPT = """Extract facts about the user from this conversation.

Only extract NEW facts that aren't already known:
{existing_memories}

Conversation:
{transcript}

Return as JSON array of memory objects.
"""
```

## Embedding Generation

### Generate Embeddings

```python
from utils.llm.clients import get_embedding_model

async def generate_embedding(text: str) -> list[float]:
    """Generate vector embedding for text."""
    model = get_embedding_model()
    response = await model.embeddings.create(
        model="text-embedding-3-large",
        input=text,
    )
    return response.data[0].embedding
```

### Batch Embeddings

```python
async def generate_embeddings_batch(texts: list[str]) -> list[list[float]]:
    """Generate embeddings for multiple texts."""
    model = get_embedding_model()
    response = await model.embeddings.create(
        model="text-embedding-3-large",
        input=texts,
    )
    return [item.embedding for item in response.data]
```

## LangGraph Patterns

### Router Configuration

```python
from langgraph.graph import StateGraph

# Define state
class ChatState(TypedDict):
    messages: list
    tools: list
    context: dict

# Create graph
graph = StateGraph(ChatState)

# Add nodes
graph.add_node("classify", classify_question)
graph.add_node("simple", simple_response)
graph.add_node("agentic", agentic_response)
graph.add_node("persona", persona_response)

# Add edges
graph.add_conditional_edges(
    "classify",
    route_question,
    {
        "simple": "simple",
        "agentic": "agentic",
        "persona": "persona",
    }
)
```

### Tool Calling

```python
from langchain.tools import tool

@tool
def get_conversations_tool(
    start_date: str,
    end_date: str,
    limit: int = 10,
) -> str:
    """Get conversations in date range."""
    conversations = await get_conversations(
        uid=uid,
        start_date=start_date,
        end_date=end_date,
        limit=limit,
    )
    return json.dumps([c.dict() for c in conversations])
```

## Error Handling

### LLM Errors

```python
from openai import APIError, RateLimitError

try:
    response = await client.chat.completions.create(...)
except RateLimitError:
    # Handle rate limit
    await asyncio.sleep(1)
    # Retry
except APIError as e:
    logger.error(f"OpenAI API error: {e}")
    raise HTTPException(status_code=500, detail="LLM error")
```

## Best Practices

1. **Model selection**: Use appropriate model for task (mini for classification, full for generation)
2. **Prompt clarity**: Write clear, specific prompts
3. **Error handling**: Handle API errors gracefully
4. **Rate limiting**: Respect rate limits and implement backoff
5. **Token management**: Monitor token usage and context length
6. **Citations**: Always cite sources in chat responses
7. **Tool descriptions**: Write clear tool descriptions for LangGraph

## Related Documentation

**The `docs/` folder is the single source of truth for all user-facing documentation, deployed at [docs.omi.me](https://docs.omi.me/).**

- **Chat System**: `docs/doc/developer/backend/chat_system.mdx` - [View online](https://docs.omi.me/doc/developer/backend/chat_system)
- **Backend Deep Dive**: `docs/doc/developer/backend/backend_deepdive.mdx` - [View online](https://docs.omi.me/doc/developer/backend/backend_deepdive)
- **Retrieval System**: `backend/utils/retrieval/`

## Related Cursor Resources

### Rules
- `.cursor/rules/backend-architecture.mdc` - System architecture and module hierarchy
- `.cursor/rules/backend-api-patterns.mdc` - API endpoint patterns for chat
- `.cursor/rules/backend-database-patterns.mdc` - Database patterns for memory storage
- `.cursor/rules/backend-imports.mdc` - Import rules for LLM modules

### Skills
- `.cursor/skills/omi-backend-patterns/` - Backend patterns including LLM integration

### Subagents
- `.cursor/agents/backend-llm-engineer/` - LLM integration and LangGraph specialist

### Commands
- `/backend-setup` - Setup LLM API keys
- `/backend-test` - Test LLM integrations
