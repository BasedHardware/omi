---
description: "Guidelines for providing complete context upfront in PRs and communications"
alwaysApply: true
---

# Context and Communication

Based on PR #3567 feedback: "show me enough info so that if you were the reviewer, you wouldn't need to ask."

## Core Principle

Provide all relevant information upfront. Don't make reviewers ask questions - anticipate what they need to know.

## PR Description Requirements

### 1. What Changed

**Clearly describe what changed.**

**Include**:
- List of files modified
- New files created
- Key changes in each file
- Architecture changes (if any)

**Example**:
```markdown
## Changes

### New Files
- `backend/utils/stt/diarization_service.py` - Orchestration layer for diarization
- `backend/routers/diarization.py` - Webhook endpoint for diarization callbacks

### Modified Files
- `backend/main.py` - Added diarization router
- `backend/utils/conversations/process_conversation.py` - Trigger refinement after save
```

### 2. Why

**Explain the problem and why this solution.**

**Include**:
- Problem being solved
- Why this approach was chosen
- Alternatives considered (if applicable)
- Design decisions and rationale

**Example**:
```markdown
## Why

This addresses Issue #2806's requirement for "speaker labelling error < 50%" of current rate.

**Why this approach**:
- Non-blocking: Users get Deepgram transcription immediately
- GPU-accelerated: Pyannote runs on Modal T4 GPU (~15-25s vs 3min on CPU)
- Leverages existing infra: Modal already used for speaker identification
- Clean integration: Follows existing webhook patterns
```

### 3. How Verified

**Show how you verified it works.**

**Include**:
- Testing performed
- Test results
- Manual testing steps
- End-to-end verification
- Performance benchmarks (if applicable)

**Example**:
```markdown
## How I Verified

### Testing
- [x] Unit tests pass
- [x] Integration tests pass
- [x] End-to-end flow tested

### Manual Testing
1. Started conversation
2. Verified Deepgram transcription works
3. Verified Pyannote refinement triggers after save
4. Verified webhook updates conversation

### Benchmarks
Tested on 4-minute Omi recording (3 speakers):
- Deepgram DER: 51.8%
- Pyannote DER: 18.1%
- Improvement: 65% better ✅
```

### 4. Related Code

**Link to relevant files and functions.**

**Include**:
- Related files
- Key functions
- Related PRs or issues
- Architecture documentation

**Example**:
```markdown
## Related Code

- `backend/utils/conversations/process_conversation.py::process_conversation()` - Main processing function
- `backend/utils/stt/diarization_service.py::trigger_diarization_refinement()` - Triggers refinement
- `backend/routers/diarization.py::webhook_callback()` - Webhook endpoint
- Related issue: #2806
- Architecture: `docs/doc/developer/backend/backend_deepdive.mdx` - [View online](https://docs.omi.me/doc/developer/backend/backend_deepdive)
```

### 5. Assumptions

**State what you assumed and how you verified.**

**Include**:
- Assumptions made
- How you verified each assumption
- What you checked before implementing

**Example**:
```markdown
## Assumptions

- **Assumed**: Audio is stored at `{uid}/{conversation_id}.wav` in GCS
  - **Verified**: Checked `backend/utils/other/storage.py::upload_conversation_recording()`
  
- **Assumed**: `process_conversation()` is still the main processing function
  - **Verified**: Searched codebase, confirmed it's still used
  
- **Assumed**: Modal webhook pattern is still valid
  - **Verified**: Checked existing Modal functions for pattern
```

### 6. Breaking Changes

**Document any breaking changes.**

**Include**:
- API changes
- Database schema changes
- Configuration changes
- Migration steps (if needed)

**Example**:
```markdown
## Breaking Changes

None - this is a new feature that doesn't break existing functionality.
```

## Communication Best Practices

### Be Proactive

**Don't wait for questions - provide answers upfront.**

**Before**:
```markdown
## Description
Added diarization refinement feature.
```

**After**:
```markdown
## Description
Added post-processing speaker diarization using Pyannote 3.1 to improve speaker labeling accuracy by 65% (confusion rate: 40.7% → 14.2%).

## Architecture
Deepgram (real-time) → Conversation Saved → Background Job
↓ Modal (T4 GPU) + Pyannote
↓ Webhook → Update DB

## How It Works
1. Conversation completes with Deepgram's real-time diarization
2. Background thread triggers `trigger_diarization_refinement()`
3. Modal function spawned with audio URL and Deepgram words
4. Pyannote runs on T4 GPU, refines speaker assignments
5. Webhook callback updates conversation with refined transcript
6. Database updated with `diarization_refined: true` flag

## Testing
- [x] End-to-end flow tested
- [x] Benchmarks provided (65% improvement)
- [x] Works with existing conversation processing
```

### Provide Evidence

**Include proof, not just claims.**

**Bad**:
```markdown
This improves performance significantly.
```

**Good**:
```markdown
Benchmark results on 4-minute recording:
- Before: 51.8% DER
- After: 18.1% DER
- Improvement: 65% better ✅
```

### Link Everything

**Make it easy for reviewers to find related information.**

**Include**:
- Links to related files
- Links to related issues/PRs
- Links to documentation
- Links to test results

### Explain Decisions

**Don't just show what - explain why.**

**Bad**:
```markdown
Used Pyannote for diarization.
```

**Good**:
```markdown
Used Pyannote for diarization because:
- 65% better accuracy than Deepgram alone
- Can run as post-processing (non-blocking)
- Leverages existing Modal infrastructure
- Follows existing webhook patterns
```

## PR Description Template

Use this template for PR descriptions:

```markdown
## Description
Brief description of what changed and why.

## Related Issue
Closes #123

## Architecture
[If applicable] Describe the architecture or design.

## Changes
- List of specific changes
- Files modified
- New files created

## How I Verified
- Testing performed
- Test results
- Manual testing steps
- Benchmarks (if applicable)

## Related Code
- Links to relevant files
- Key functions
- Related PRs/issues

## Assumptions
- Assumption 1: Verified by X
- Assumption 2: Verified by Y

## Breaking Changes
[If any] Describe breaking changes and migration steps.

## Testing
- [ ] Tests pass
- [ ] Manual testing completed
- [ ] Works in all relevant states
- [ ] Performance verified (if applicable)
```

## Common Gaps

**What reviewers often need to ask about**:

1. **How it works**: Not explaining the flow
2. **Why this approach**: Not explaining decisions
3. **Testing**: Not showing how it was verified
4. **Assumptions**: Not stating what was assumed
5. **Related code**: Not linking to relevant files
6. **Edge cases**: Not mentioning edge cases handled

**How to avoid**:
- Explain the complete flow
- Explain why you chose this approach
- Show evidence of testing
- State assumptions explicitly
- Link to all relevant code
- Mention edge cases handled

## Related Rules

- `.cursor/rules/pre-implementation-checklist.mdc` - Pre-implementation verification
- `.cursor/rules/common-mistakes.mdc` - Common mistakes to avoid
- `.cursor/rules/verification.mdc` - Self-checking guidelines
- `.cursor/rules/git-workflow.mdc` - Git workflow including PR process

## Related Cursor Resources

### Commands
- `/pr` - Create pull request with proper description
- `/code-review` - Review code before PR

### Skills
- `.cursor/skills/self-improvement/SKILL.md` - Learn from PRs and issues
